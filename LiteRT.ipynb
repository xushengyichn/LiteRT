{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK5EhGGNMOJh"
      },
      "source": [
        "# LiteRT using Google AI Edge for on-device object detection\n",
        "This notebook is an implementation of converting the YOLO11 object detection model to LiteRT (.tflite) format using Google AI Edge and deploy it on Android for on-device inference.\n",
        "\n",
        "Developed by [Levi Lin](https://github.com/gy6543721)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXou4eK436nb"
      },
      "source": [
        "#### Step 1: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNzOT6QeJo-1"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "!pip install ai-edge-model-explorer\n",
        "!pip install ai-edge-litert\n",
        "!pip install onnx2tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEQRsPqv6GCc"
      },
      "source": [
        "#### Step 2: Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB12S2E7J0O8"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ai_edge_litert.interpreter import Interpreter\n",
        "from google.colab import files\n",
        "\n",
        "import model_explorer\n",
        "import yaml\n",
        "import json\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgFnfd3K6-aS"
      },
      "source": [
        "#### Step 3: Convert YOLO11 model to LiteRT (TF Lite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b03zjtZkzZjz"
      },
      "outputs": [],
      "source": [
        "# Load the YOLO11 model.\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "# Export the model to LiteRT (TF Lite) format.\n",
        "model.export(format=\"tflite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WII5g-lHXdu"
      },
      "source": [
        "Download a sample image or load your own image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip6wk1nPHX6O"
      },
      "outputs": [],
      "source": [
        "# Download sample image and video.\n",
        "!wget https://raw.githubusercontent.com/gy6543721/LiteRT/main/assets/test_image.jpg\n",
        "!wget https://raw.githubusercontent.com/gy6543721/LiteRT/main/assets/test_image_2.jpg\n",
        "!wget https://raw.githubusercontent.com/gy6543721/LiteRT/main/assets/test_video.mp4\n",
        "\n",
        "image = Image.open('test_image_2.jpg')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYOur8jdU1n8"
      },
      "outputs": [],
      "source": [
        "LITE_RT_EXPORT_PATH = \"yolo11n_saved_model/\" # @param {type : 'string'}\n",
        "LITE_RT_MODEL = \"yolo11n_float32.tflite\" # @param {type : 'string'}\n",
        "\n",
        "LITE_RT_MODEL_PATH = LITE_RT_EXPORT_PATH + LITE_RT_MODEL\n",
        "\n",
        "# Load the exported TF Lite model.\n",
        "litert_model = YOLO(LITE_RT_MODEL_PATH, task = 'detect')\n",
        "\n",
        "# Input image.\n",
        "image = 'test_image_2.jpg' # @param {type : 'string'}\n",
        "\n",
        "# Perform inference on the input image.\n",
        "result = litert_model(image)\n",
        "result[0].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxoxnVgMM2Wd"
      },
      "source": [
        "#### Step 4: Visualize the LiteRT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj3HbJB1NMZ9"
      },
      "outputs": [],
      "source": [
        "model_explorer.visualize(LITE_RT_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJm6c5kh-LtC"
      },
      "source": [
        "#### Step 5: Create labelmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRmCeCwWM2nI",
        "outputId": "a3ce1bc3-3a4d-49de-f21e-08d08f441a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labelmap created.\n"
          ]
        }
      ],
      "source": [
        "metadata_file = \"metadata.yaml\" # @param {type : 'string'}\n",
        "json_file = \"labels.json\" # @param {type : 'string'}\n",
        "\n",
        "metadata_path = LITE_RT_EXPORT_PATH + metadata_file\n",
        "\n",
        "with open(metadata_path, \"r\") as file:\n",
        "    metadata = yaml.safe_load(file)\n",
        "\n",
        "names = metadata.get(\"names\", {})\n",
        "\n",
        "with open(json_file, 'w') as file:\n",
        "  json.dump(names, file, indent=2)\n",
        "\n",
        "print(\"Labelmap created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uGY6Z2b_ruS"
      },
      "source": [
        "#### Step 6: Inference the TF Lite model using LiteRT interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0RrQKBWAdl7"
      },
      "outputs": [],
      "source": [
        "# Load the TF Lite model.\n",
        "interpreter = Interpreter(model_path = LITE_RT_MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_size = input_details[0]['shape'][1]\n",
        "\n",
        "print(f\"Model input size: {input_size}\")\n",
        "print(f\"Output tensor shape: {output_details[0]['shape']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhcdQCUPAjkK"
      },
      "source": [
        "#### Step 7: Define utility functions\n",
        "\n",
        "`load_labels`: Loads the `labels.json` file.\n",
        "\n",
        "`load_image`: Loads the input image.\n",
        "\n",
        "`detect`: Run the LiteRT model.\n",
        "\n",
        "`postprocess_output`: Normalize the bounding box coordinates.\n",
        "\n",
        "`generate_color_map`: Generates unique colors randomly for each label.\n",
        "\n",
        "`inference_image`: Inference detection on images.\n",
        "\n",
        "`inference_video`: Inference detection on videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "vCvrq4uPA9QK"
      },
      "outputs": [],
      "source": [
        "# Load labels.\n",
        "def load_labels(label_file):\n",
        "  with open(label_file, 'r') as file:\n",
        "    return json.load(file)\n",
        "\n",
        "\n",
        "# Load and preprocess image.\n",
        "def load_image(image_path, input_size):\n",
        "  image = cv2.imread(image_path)\n",
        "  original_height, original_width = image.shape[:2]\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image, (input_size, input_size))\n",
        "  image = image / 255.0\n",
        "  return image, (original_height, original_width)\n",
        "\n",
        "\n",
        "# Run inference.\n",
        "def detect(input_data, is_video_frame=False):\n",
        "    input_size = input_details[0]['shape'][1]\n",
        "\n",
        "    if is_video_frame:\n",
        "        original_height, original_width = input_data.shape[:2]\n",
        "        image = cv2.cvtColor(input_data, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (input_size, input_size))\n",
        "        image = image / 255.0\n",
        "    else:\n",
        "        image, (original_height, original_width) = load_image(input_data, input_size)\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(image, axis=0).astype(np.float32))\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output_data = [interpreter.get_tensor(detail['index']) for detail in output_details]\n",
        "    return output_data, (original_height, original_width)\n",
        "\n",
        "\n",
        "\n",
        "# Postprocess the output.\n",
        "def postprocess_output(output_data, original_dims, labels, confidence_threshold):\n",
        "  output_tensor = output_data[0]\n",
        "  detections = []\n",
        "  original_height, original_width = original_dims\n",
        "\n",
        "  for i in range(output_tensor.shape[1]):\n",
        "    box = output_tensor[0, i, :4]\n",
        "    confidence = output_tensor[0, i, 4]\n",
        "    class_id = int(output_tensor[0, i, 5])\n",
        "\n",
        "    if confidence > confidence_threshold:\n",
        "      x_min = int(box[0] * original_width)\n",
        "      y_min = int(box[1] * original_height)\n",
        "      x_max = int(box[2] * original_width)\n",
        "      y_max = int(box[3] * original_height)\n",
        "\n",
        "      label_name = labels.get(str(class_id), \"Unknown\")\n",
        "\n",
        "      detections.append({\n",
        "          \"box\": [y_min, x_min, y_max, x_max],\n",
        "          \"score\": confidence,\n",
        "          \"class\": class_id,\n",
        "          \"label\": label_name\n",
        "      })\n",
        "\n",
        "  return detections\n",
        "\n",
        "\n",
        "# Generate color map for labels.\n",
        "def generate_color_map(labels):\n",
        "  color_map = {}\n",
        "  for label in labels.values():\n",
        "      color_map[label] = [random.randint(0, 255) for _ in range(3)]\n",
        "  return color_map\n",
        "\n",
        "\n",
        "# Inference on image.\n",
        "def inference_image(image_path, detections, color_map):\n",
        "  image = cv2.imread(image_path)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  for detection in detections:\n",
        "    box = detection['box']\n",
        "    label = detection['label']\n",
        "    score = detection['score']\n",
        "    color = color_map[label]\n",
        "\n",
        "    y_min, x_min, y_max, x_max = box\n",
        "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 3)\n",
        "\n",
        "    text = f'{label}: {score:.2f}'\n",
        "    font_scale = 1\n",
        "    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 3)[0]\n",
        "\n",
        "    label_start = (x_min, y_min - text_size[1] - 10)\n",
        "    label_end = (x_min + text_size[0], y_min)\n",
        "\n",
        "    cv2.rectangle(image, label_start, label_end, color, -1)\n",
        "\n",
        "    text_position = (x_min, y_min - 5)\n",
        "    cv2.putText(image, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), 2)\n",
        "\n",
        "  output_image_path = 'output_' + image_path\n",
        "  output_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Show the image.\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  return output_image\n",
        "\n",
        "\n",
        "# Inference on video.\n",
        "def inference_video(frame, detections, color_map, out):\n",
        "    # Convert the frame to RGB for processing.\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    for detection in detections:\n",
        "        box = detection['box']\n",
        "        label = detection['label']\n",
        "        score = detection['score']\n",
        "        color = color_map[label]\n",
        "\n",
        "        y_min, x_min, y_max, x_max = box\n",
        "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 3)\n",
        "\n",
        "        text = f'{label}: {score:.2f}'\n",
        "        font_scale = 0.5\n",
        "        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 3)[0]\n",
        "\n",
        "        label_start = (x_min, y_min - text_size[1] - 10)\n",
        "        label_end = (x_min + text_size[0], y_min)\n",
        "\n",
        "        cv2.rectangle(image, label_start, label_end, color, -1)\n",
        "\n",
        "        text_position = (x_min, y_min - 5)\n",
        "        cv2.putText(image, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), 1)\n",
        "\n",
        "    output_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    out.write(output_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_EEZvtGOKEM"
      },
      "source": [
        "#### Step 8: Visualize the inference on image and video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33dgKKi_OP9W"
      },
      "outputs": [],
      "source": [
        "label_file = 'labels.json' # @param {type : 'string'}\n",
        "input_type = 'image' # @param ['image', 'video']\n",
        "image_path = 'test_image_2.jpg' # @param {type : 'string'}\n",
        "video_path = 'test_video.mp4' # @param {type : 'string'}\n",
        "confidence_threshold = 0.4 # @param {type : 'slider', min:0, max:1, step: 0.1}\n",
        "\n",
        "labels = load_labels(label_file)\n",
        "color_map = generate_color_map(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax_yf-MyXe9A"
      },
      "outputs": [],
      "source": [
        "if input_type == 'image':\n",
        "    output_data, original_dims = detect(image_path)\n",
        "    detections = postprocess_output(output_data, original_dims, labels, confidence_threshold)\n",
        "    output_img = inference_image(image_path, detections, color_map)\n",
        "    output_image = 'output_' + image_path\n",
        "    cv2.imwrite(output_image, output_img)\n",
        "    print(f\"Image saved as {output_image}\")\n",
        "\n",
        "else:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    output_video = 'output_' + video_path.replace('mp4', 'avi')\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        output_data, original_dims = detect(frame, is_video_frame=True)\n",
        "        detections = postprocess_output(output_data, original_dims, labels, confidence_threshold)\n",
        "\n",
        "        inference_video(frame, detections, color_map, out)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"Output video saved as {output_video}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P7ajg4CYWXA"
      },
      "source": [
        "#### Step 9: Download output image and video (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8Bt54ZSaYchX",
        "outputId": "f88e1aa1-403c-42f4-a298-8d41e1beedcf"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_937c37ce-6caf-45ec-9da6-2270a98d58b3\", \"output_test_image.jpg\", 277201)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a9ab6946-59a1-42a2-bcd4-9b10828b2f80\", \"output_Leh.avi\", 74489708)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download output image.\n",
        "files.download(output_image)\n",
        "\n",
        "# Download output video.\n",
        "files.download(output_video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84mBrktoJZiR"
      },
      "source": [
        "#### Step 10: Download the LiteRT model\n",
        "\n",
        "Download the exported LiteRT model for on-device deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BbRaAfFbIy6b",
        "outputId": "c9eeda29-530f-46ed-81dc-b7d114927205"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c172f3d3-ea2a-45f7-8fc6-119e8b003b6d\", \"yolov10n_float16.tflite\", 5269098)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download(LITE_RT_MODEL_PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}